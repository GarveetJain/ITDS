{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea060d7",
   "metadata": {},
   "source": [
    "## Logistic loss\n",
    "Logistic loss measures how well a model’s predicted probabilities match the actual binary outcomes (0 or 1). It penalizes confident but incorrect predictions very heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logistic_loss(Y, y_pred_proba):\n",
    "    loss = -(Y * np.log(y_pred_proba) + (1 - Y) * np.log(1 - y_pred_proba))\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45564b6",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "Model calibration refers to how well a model’s predicted probabilities reflect the true likelihood of outcomes.\n",
    "\n",
    "In a well-calibrated model, predictions match observed frequencies.\n",
    "\n",
    "Calibration model is learning how to model: raw probability → true empirical probability\n",
    "\n",
    "1. Train base classifier on training data\n",
    "2. Predict probabilities on calibration data\n",
    "3. Fit isotonic or logistic calibrator (X - predicted probs ; Y - true class)\n",
    "4. Apply calibrator to test predictions\n",
    "5. Evaluate using Brier score or calibration curve\n",
    "6. Optionally compute classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbe336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for calibrating a model\n",
    "model = (...)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_calib)\n",
    "\n",
    "calib_model = (...)\n",
    "calib_model.fit(y_pred_proba.reshape(-1, 1), Y_calib)\n",
    "\n",
    "pred_raw = model.predict_proba(X_test)\n",
    "pred_calib = calib_model.predict_proba(pred_raw.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4999fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "prob_true_raw, prob_pred_raw = calibration_curve(Y_test, raw_probs, n_bins=10, strategy='uniform')\n",
    "prob_true_cal, prob_pred_cal = calibration_curve(Y_test, calibrated_probs, n_bins=10, strategy='uniform')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(prob_pred_raw, prob_true_raw, marker='o', label='Raw model')\n",
    "plt.plot(prob_pred_cal, prob_true_cal, marker='s', label='Calibrated model')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Observed fraction of positives')\n",
    "plt.title('Calibration Plot / Reliability Diagram')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614cc04a",
   "metadata": {},
   "source": [
    "## Confidence Intervals\n",
    "Calculating confidence intervals using:\n",
    "- CLT\n",
    "- Markov's inequality\n",
    "- Chebychev's ineugality\n",
    "- Hoeffding's inequality\n",
    "- Bennet's inequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b607c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean: 6.444444444444445\n",
      "95% CI: (np.float64(5.163331035997686), np.float64(7.725557852891203))\n"
     ]
    }
   ],
   "source": [
    "# CLT\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def conf_interval(data, conf_level = 0.95):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data, ddof=1)\n",
    "\n",
    "    # CI using t-distribution\n",
    "    alpha = 1 - conf_level\n",
    "    t_crit = stats.t.ppf(1 - alpha/2, df=n-1)\n",
    "\n",
    "    margin_error = t_crit * std / np.sqrt(n)\n",
    "    ci = (mean - margin_error, mean + margin_error)\n",
    "    print(\"Sample mean:\", mean)\n",
    "    print(\"95% CI:\", ci)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1413579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov's inequality\n",
    "import numpy as np\n",
    "\n",
    "def markov_conf_interval(data, conf_level=0.95):\n",
    "    n = len(data)\n",
    "\n",
    "    sample_mean = np.mean(data)\n",
    "    alpha = 1 - conf_level\n",
    "    upper_bound = sample_mean / alpha\n",
    "\n",
    "    return upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4261f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebychev_conf_interval(data, conf_level=0.95):\n",
    "    n = len(data)\n",
    "    alfa = 1 - conf_level\n",
    "    epsilon = np.sqrt(np.var(data)/(n*alfa))\n",
    "    mean = np.mean(data)\n",
    "\n",
    "    return (mean - epsilon, mean + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff8a0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hoeffding_conf_interval(data, a, b, conf_level=0.95):\n",
    "    alpha = 1 - conf_level\n",
    "    n = len(data)\n",
    "    epsilon = (b - a) / np.sqrt(2 * n) * np.sqrt(np.log(2 / alpha))\n",
    "    mean = np.mean(data)\n",
    "    return (mean - epsilon, mean + epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e3e7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bennet_conf_interval(data, conf_level=0.95):\n",
    "    variance = np.var(data)\n",
    "    alfa = 1 - conf_level\n",
    "    n = len(data)\n",
    "\n",
    "    epsilon = np.sqrt( (2 * variance * np.log(2/alfa)) / n ) + (5 * np.log(2/alfa)) / (3 * n)\n",
    "    mean = np.mean(data)\n",
    "\n",
    "    interval = (mean - epsilon, mean + epsilon)\n",
    "    return interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def makeFreq(data_sequence):\n",
    "    \"\"\"\n",
    "    Takes a data_sequence in the form of iterable and returns a\n",
    "    numpy array of the form [keys,counts] where the keys\n",
    "    are the unique values in data_sequence and counts are how\n",
    "    many time they appeared\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    data = np.array(data_sequence)\n",
    "    if (len(data.shape) == 2):\n",
    "        (keys,counts) = np.unique(data.T,axis=0,return_counts=True)\n",
    "        return np.concatenate([keys,counts.reshape(-1,1)],axis=1)\n",
    "    else:\n",
    "        (keys,counts) = np.unique(data,return_counts=True)\n",
    "        return np.stack([keys,counts],axis=-1)\n",
    "\n",
    "def makeEDF(data_sequence):\n",
    "    import numpy as np\n",
    "    numRelFreqPairs = makeFreq(data_sequence)\n",
    "    (keys,counts) = (numRelFreqPairs[:,0],numRelFreqPairs[:,1])\n",
    "    frequencies = counts/np.sum(counts)\n",
    "    emf = np.stack([keys,frequencies],axis=-1)\n",
    "    cumFreqs = np.cumsum(frequencies)\n",
    "    edf = np.stack([keys,cumFreqs],axis=-1)\n",
    "\n",
    "    return edf\n",
    "\n",
    "def DKW_conf_interval(data, conf_level=0.95):\n",
    "    edf = makeEDF(data.reshape(-1))\n",
    "    alpha = 1 - conf_level\n",
    "    n = len(data)\n",
    "    epsilon = np.sqrt(np.log(2 / alpha) / (2 * n))\n",
    "    lower = np.maximum(0, edf - epsilon)\n",
    "    upper = np.minimum(1, edf + epsilon)\n",
    "    return (lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9de5fa",
   "metadata": {},
   "source": [
    "## Markov chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02982ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating Stationary Distribution\n",
    "import numpy as np\n",
    "\n",
    "def stationary_dist(P):\n",
    "    P = np.array(P)\n",
    "    eigenvals, eigenvecs = np.linalg.eig(P.T)\n",
    "\n",
    "    # find the index of the eigenvalue that is 1\n",
    "    index = np.argmin(np.abs(eigenvals - 1))\n",
    "\n",
    "    stat = eigenvecs[:, index].flatten() # getting eigenvec with eigenval 1\n",
    "    stat = stat / np.sum(stat) #normalizing vector\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5111ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating if the MC is irreducible\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def is_irreducible(P):\n",
    "    G = nx.DiGraph()\n",
    "    n = P.shape[0]\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if P[i, j] > 0:\n",
    "                G.add_edge(i, j)\n",
    "    \n",
    "    return nx.is_strongly_connected(G)\n",
    "\n",
    "def is_irreducible_nx(matrix):\n",
    "    # Convert numpy array to a NetworkX Directed Graph\n",
    "    G = nx.from_numpy_array(matrix, create_using=nx.DiGraph)\n",
    "    \n",
    "    # Check if the graph is strongly connected\n",
    "    return nx.is_strongly_connected(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6557f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating if the MC is aperiodic\n",
    "from math import gcd\n",
    "from functools import reduce\n",
    "\n",
    "def is_aperiodic(P, max_power=100):\n",
    "    n = P.shape[0]\n",
    "    powers = np.eye(n)\n",
    "    return_times = []\n",
    "\n",
    "    for k in range(1, max_power + 1):\n",
    "        powers = powers @ P\n",
    "        if powers[0, 0] > 0:  # return to state 0\n",
    "            return_times.append(k)\n",
    "\n",
    "    if len(return_times) == 0:\n",
    "        return False\n",
    "\n",
    "    period = reduce(gcd, return_times)\n",
    "    return period == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating if the MC is reversible\n",
    "def is_reversible(P, tol=1e-8):\n",
    "\n",
    "    if not (is_irreducible(P) and is_aperiodic(P)): # if there is no stationary distribution\n",
    "        return False\n",
    "    \n",
    "    P = np.array(P)\n",
    "    n = P.shape[0]\n",
    "\n",
    "    pi = stationary_dist(P)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if not np.isclose(pi[i] * P[i, j], pi[j] * P[j, i], atol=tol):\n",
    "                return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd182f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating n-step transition matrix\n",
    "import numpy as np\n",
    "\n",
    "def n_step_transition(P, n):\n",
    "    '''\n",
    "    Calculating n-step transition matrix.\n",
    "    P^n --> transitions after n steps.\n",
    "    With increasing n, each row will converge to stationary dist.\n",
    "    '''\n",
    "    return np.linalg.matrix_power(P, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd79a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_transition_matrix(transitions, n_states):\n",
    "    \"\"\"\n",
    "    Estimate the transition matrix P from observed transitions.\n",
    "\n",
    "    Args:\n",
    "        transitions: array of shape (n_samples, 2)\n",
    "        n_states: number of states\n",
    "\n",
    "    Returns:\n",
    "        P_hat: estimated transition matrix\n",
    "    \"\"\"\n",
    "    P_hat = np.zeros((n_states, n_states))\n",
    "    \n",
    "    n_transitions = transitions.shape[0]\n",
    "    for t in transitions:\n",
    "        \n",
    "        P_hat[t[0],t[1]] += 1\n",
    "\n",
    "    row_sums = P_hat.sum(axis=1)\n",
    "    P_hat = P_hat / row_sums[:, np.newaxis]\n",
    "\n",
    "    return P_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_transition_matrix(P):\n",
    "    \"\"\"\n",
    "    Check if P is a transition matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    if P.ndim != 2 or P.shape[0] != P.shape[1]:\n",
    "        print(\"Wrong shape\")\n",
    "        return False\n",
    "\n",
    "    if not np.isfinite(P).all():\n",
    "        print(\"Is not finite\")\n",
    "        return False\n",
    "\n",
    "    if np.all(P < 0):\n",
    "        print(\"Has negative elements\")\n",
    "        return False\n",
    "\n",
    "    if not np.allclose(P.sum(axis=1), 1.0, atol=1e-8):\n",
    "        print(\"Rows do not sum to 1\")\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8792b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_chain(P, start_state, n_steps):\n",
    "    \"\"\"\n",
    "    Simulate a Markov chain trajectory with a fixed random seed.\n",
    "\n",
    "    Returns: array of visited states of length n_steps + 1\n",
    "    \"\"\"\n",
    "    # seed = 1234  # don't change that\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    path = np.zeros(n_steps + 1, dtype=int)\n",
    "    path[0] = start_state\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        path[t + 1] = rng.choice(\n",
    "            P.shape[0],\n",
    "            p=P[path[t]]\n",
    "        )\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcad7c6",
   "metadata": {},
   "source": [
    "## Linear Congruential Generator LCG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b507dd",
   "metadata": {},
   "source": [
    "This is a  simple way to generate chains of pseudorandom numbers and uniform distributions\n",
    "\n",
    "A good way to create an LCG is to follow the hull dobell theorem\n",
    " ### Hull–Dobell Theorem.\n",
    "\n",
    "The congruential generator (a,b,M) has period M iff\n",
    "\n",
    "1. gcd(b,M)=1,\n",
    "\n",
    "2. p divides a-1 for every prime p that divides M\n",
    "\n",
    "3. 4 divides a- 1 if 4 divides M.\n",
    "\n",
    "some good numbers for LCG:\n",
    "\n",
    "m = 2^31 - 1 = 2147483647\n",
    "\n",
    "a=16807\n",
    "\n",
    "c = 0\n",
    "\n",
    "seed = {1, …, m-1}\n",
    "\n",
    "Xn+1 = a*Xn mod (2^31 - 1)\n",
    "IF WE WANT UNIFORM NUMBERS Un = Xn/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c72b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linConGen(n, m, a, b, x0):\n",
    "    '''A linear congruential sequence generator.\n",
    "    \n",
    "    Param m is the integer modulus to use in the generator.\n",
    "    Param a is the integer multiplier.\n",
    "    Param b is the integer increment.\n",
    "    Param x0 is the integer seed.\n",
    "    Param n is the integer number of desired pseudo-random numbers.\n",
    "    \n",
    "    Returns a list of n pseudo-random integer modulo m numbers.'''\n",
    "    \n",
    "    x = x0 # the seed\n",
    "    retValue = [x % m]  # start the list with x=x0\n",
    "    for i in range(2, n+1, 1):\n",
    "        x = (a * x + b) % m # the generator, using modular arithmetic\n",
    "        retValue.append(x) # append the new x to the list\n",
    "    return retValue\n",
    "\n",
    "def uniformFromLCG(n,m,a,b,x0):\n",
    "  return linConGen(n,m,a,b,x0) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e345220",
   "metadata": {},
   "source": [
    "## Accept-Reject Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e66cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem1_inversion(f, M, n_samples=1):\n",
    "\n",
    "    if M is None: # finding max of a function\n",
    "        from scipy.optimize import minimize_scalar\n",
    "        res = minimize_scalar(lambda x: -f(x), bounds=(0, 1), method='bounded')\n",
    "        M = f(res.x)\n",
    "\n",
    "    result = []\n",
    "    while (len(result) < n_samples):\n",
    "        x1 = np.random.uniform(0,1)\n",
    "        f_x = f(x1)\n",
    "        x2 = np.random.uniform(0,1)\n",
    "        if (x2 <= f_x/M):\n",
    "            result.append(x1)\n",
    "        else:\n",
    "            continue\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b2a5a",
   "metadata": {},
   "source": [
    "# Inverse Transform Sampling ITS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e12f55f",
   "metadata": {},
   "source": [
    "For an exponential distribution with parameter $\\lambda = 1$, the Probability Density Function (PDF) is given by:\n",
    "\n",
    "$f(x) = e^{-x}$ for $x \\ge 0$\n",
    "\n",
    "The Cumulative Distribution Function (CDF) is found by integrating the PDF:\n",
    "\n",
    "$F(x) = \\int_{0}^{x} e^{-t} dt = [-e^{-t}]_{0}^{x} = -e^{-x} - (-e^{-0}) = 1 - e^{-x}$\n",
    "\n",
    "To find the Inverse CDF, we set $F(x) = u$, where $u$ is a random variable uniformly distributed between 0 and 1, and solve for $x$:\n",
    "\n",
    "$u = 1 - e^{-x}$\n",
    "\n",
    "$e^{-x} = 1 - u$\n",
    "\n",
    "$-x = \\ln(1 - u)$\n",
    "\n",
    "$x = -\\ln(1 - u)$\n",
    "\n",
    "This formula allows us to generate samples from an exponential distribution using uniform random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7362401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exponential distribution with lambda = 1\n",
    "pdf_x = lambda x: np.exp(-x)\n",
    "cdf_x = lambda x: 1 - np.exp(-x)\n",
    "cdf_inverse_x = lambda u: -np.log(1 - u)\n",
    "\n",
    "Xs = np.linspace(0, 10, num = 1000)\n",
    "Ys = pdf_x(Xs)\n",
    "\n",
    "plt.plot(Xs,Ys, label = 'true distribution')\n",
    "\n",
    "\n",
    "\n",
    "#Inverse sampling of the distribution\n",
    "samples = []\n",
    "for i in range(10000):\n",
    "  u = np.random.uniform(0,1)\n",
    "  sample_x = cdf_inverse_x(u)\n",
    "  samples.append(sample_x)\n",
    "\n",
    "\n",
    "plt.hist(samples, bins = 100, density= True, label = 'samples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed556f9",
   "metadata": {},
   "source": [
    "# Box-Muller Sampling (Box-Muller Transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a382e62",
   "metadata": {},
   "source": [
    "To generate two independent samples, $Z_0$ and $Z_1$, from a Standard Normal Distribution $N(0, 1)$, follow these steps:\n",
    "\n",
    "Step 1: Generate Uniform Inputs\n",
    "\n",
    "Generate two independent random numbers, $U_1$ and $U_2$, from a uniform distribution:$$U_1, U_2 \\sim \\text{Uniform}(0, 1)$$\n",
    "\n",
    "Step 2: Apply the Box-Muller Equations\n",
    "\n",
    "Use the following transformation equations to convert the uniform samples into normal samples:\n",
    "\n",
    "$$Z_0 = \\sqrt{-2 \\ln U_1} \\cos(2\\pi U_2)$$\n",
    "\n",
    "$$Z_1 = \\sqrt{-2 \\ln U_1} \\sin(2\\pi U_2)$$\n",
    "\n",
    "Step 3: Adjust for Target Mean ($\\mu$) and Variance ($\\sigma^2$)\n",
    "\n",
    "If you need samples from a distribution $N(\\mu, \\sigma^2)$ rather than the standard $N(0, 1)$, apply the linear transformation:$$X = \\mu + Z\\sigma$$\n",
    "\n",
    "Summary of the mapping:\n",
    "\n",
    "The term $\\sqrt{-2 \\ln U_1}$ generates the magnitude (radius).The term $2\\pi U_2$ generates the angle (0 to 360 degrees).The $\\cos$ and $\\sin$ functions project that magnitude and angle onto the $X$ and $Y$ axes to produce the final coordinates (samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def box_muller_sample(n_samples, mu=0, sigma=1):\n",
    "    \"\"\"\n",
    "    Generates normal samples using the Box-Muller transform.\n",
    "    Returns two arrays of independent samples.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate uniform random numbers U1, U2 ~ Uniform(0, 1)\n",
    "    u1 = np.random.rand(n_samples)\n",
    "    u2 = np.random.rand(n_samples)\n",
    "\n",
    "    # Step 2: Apply the transformation equations\n",
    "    # Magnitude (Radius)\n",
    "    mag = np.sqrt(-2 * np.log(u1))\n",
    "\n",
    "    # Angle (0 to 2*pi)\n",
    "    angle = 2 * np.pi * u2\n",
    "\n",
    "    # Calculate Z0 and Z1\n",
    "    z0 = mag * np.cos(angle)\n",
    "    z1 = mag * np.sin(angle)\n",
    "\n",
    "    # Step 3: Scale to target mean and standard deviation\n",
    "    x0 = mu + z0 * sigma\n",
    "    x1 = mu + z1 * sigma\n",
    "\n",
    "    return x0, x1\n",
    "\n",
    "# --- Execution and Visualization ---\n",
    "n = 10000\n",
    "samples_x, samples_y = box_muller_sample(n)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Histogram to verify Normal Distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(samples_x, bins=50, density=True, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title(f\"Histogram of Z0 samples (n={n})\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Plot Scatter to show independence/joint distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(samples_x, samples_y, s=1, alpha=0.2, color='purple')\n",
    "plt.title(\"Joint Distribution (Z0 vs Z1)\")\n",
    "plt.xlabel(\"Z0\")\n",
    "plt.ylabel(\"Z1\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d619e25",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting histograms of each column in a df\n",
    "\n",
    "n_rows, n_cols = 6, 5\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 16))  # Adjust figure size\n",
    "axes = axes.flatten()  # Flatten 2D array of axes to 1D for easy iteration\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    axes[i].hist(df[col], bins=20, color='skyblue', edgecolor='black')\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "# Hide any unused subplots if X has fewer than 16 columns\n",
    "for j in range(i+1, n_rows*n_cols):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab676e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap\n",
    "import seaborn as sns\n",
    "\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142ba15",
   "metadata": {},
   "source": [
    "# Poisson Regression - evaluation\n",
    "\n",
    "This is poisson deviance used for evaluation of Poisson Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2eff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def poisson_deviance(y_true, y_pred, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Oblicza dewiancję Poissona\n",
    "\n",
    "    y_true : array-like (prawdziwe liczby wizyt)\n",
    "    y_pred : array-like (przewidywane lambda > 0)\n",
    "    eps    : zabezpieczenie numeryczne dla log(0)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    # zabezpieczenie: lambda musi być dodatnia\n",
    "    y_pred = np.maximum(y_pred, eps)\n",
    "\n",
    "    # specjalne traktowanie y = 0 (bo 0 * log(0) = 0)\n",
    "    term = np.where(\n",
    "        y_true == 0,\n",
    "        -y_pred,\n",
    "        y_true * np.log(y_true / y_pred) - (y_true - y_pred)\n",
    "    )\n",
    "\n",
    "    return 2 * np.sum(term)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
